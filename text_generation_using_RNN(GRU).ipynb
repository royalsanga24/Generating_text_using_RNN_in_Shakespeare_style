{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "738e304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b17ab986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1115394/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa686ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8868c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c603b3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '$',\n",
       " '&',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '3',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de9e1bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "273023f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=string, numpy=array([b'F', b'i', b'r', ..., b'g', b'.', b'\\n'], dtype=object)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc390e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a861eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1], dtype=int64)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bce1ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2fb25ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f41f983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0708f1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4bdc219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80b8f13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc3bae1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb5a7951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b4ec2be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59d866f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d200b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f33858ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9874c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60d296e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.190693, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b76936ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ffa5495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8b8e0cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ebf0a55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 588s 3s/step - loss: 2.1090\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 532s 3s/step - loss: 1.7741\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 535s 3s/step - loss: 1.5825\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 544s 3s/step - loss: 1.4693\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 537s 3s/step - loss: 1.3946\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 532s 3s/step - loss: 1.3381\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 528s 3s/step - loss: 1.2937\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 524s 3s/step - loss: 1.2505\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 524s 3s/step - loss: 1.2097\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 526s 3s/step - loss: 1.1699\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 524s 3s/step - loss: 1.1296\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 522s 3s/step - loss: 1.0862\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 527s 3s/step - loss: 1.0404\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 525s 3s/step - loss: 0.9921\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 525s 3s/step - loss: 0.9416\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 525s 3s/step - loss: 0.8888\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 523s 3s/step - loss: 0.8377\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 555s 3s/step - loss: 0.7849\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 539s 3s/step - loss: 0.7354\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 536s 3s/step - loss: 0.6906\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d56fe8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./rnn_text_generation\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./rnn_text_generation\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('./rnn_text_generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "72688e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1af81baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e89d938b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "You are quite, most royal sort: I would say.\n",
      "\n",
      "HERMIONE:\n",
      "'Sigein being prison, might hept as the duegest\n",
      "Thou wert so proper-tide.\n",
      "\n",
      "LADY ANNE:\n",
      "And, I hope, I'll not only son,\n",
      "I small of his bearing a bottle have peeres,\n",
      "I would say you were aloked moods.\n",
      "\n",
      "KING RICHARD III:\n",
      "Shall I have my wish! sure, if my lord confound!\n",
      "\n",
      "HERMIONE:\n",
      "There like the rancomont\n",
      "Show'd until anointed him\n",
      "And all the tribunes proper to the goose.\n",
      "\n",
      "Nurse:\n",
      "No more, which will it had--\n",
      "Being me over harghter, will starp strict\n",
      "Than my poor hour left privilege him,\n",
      "Nor in't is but to hear; and like a bale,\n",
      "Till we can convinu and plague in my hosty;\n",
      "And takes her up in prison a moithor,\n",
      "Betwixt thy way: and we in void,\n",
      "Unhearing, she hath not, dissoveth.\n",
      "Pretty father, to thyself and quiet can rech,\n",
      "Stand night and leave unto your graces for,\n",
      "And some five and kind contenting in the world.\n",
      "\n",
      "BRUTUS:\n",
      "I think he would seek she is ready\n",
      "To heareth-revelling things in an all wholesome choice\n",
      "Sitting to deal'd a kneel, \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.295771360397339\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d7ee203a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"ROMEO:\\nThe island, wherein which shall report to the law: and, so it me!\\nAnd give us leisure me true time broke\\nThan he was gonged, come, daughter, and our king and leaves\\nAnd close struck nightly falsehood with his hage,\\nAs wilt thou object'd me in the rough,\\nWere best in love of liberty in his birth!\\n\\nJULIET:\\nSwall, with rebellious Hastings, help me his lough,\\nTo instruct her and I prove thee and Patick:\\nAnd 'twas no please to look pale prey.\\nButly, Wrayon, is nature with my lady?\\n\\nJULIET:\\nWhy, this shall give you to your daughter.\\nThe league is but a fool, invisitionable\\nWith woes, where was some preggiest deeply too.\\n\\nISABELLA:\\nMy true Pembroke? withdraw you mock me.\\n\\nPETRUCHIO:\\nNay, but the more so falls but this: good lords, boy,--before unto your commonar.\\nChare, Lord Convitionable against your bawd,\\nAnd given to wail and to by skulls; and,\\nThat time when his does fierce be furnis,' says he\\nuttered, my liege, the news? and, methinks, to scorn us in his acts,\\nIs all will eat on runn, f\"\n",
      " b\"ROMEO:\\nThou shalt be that his knees had plot'd the earth hastes,\\nAnd with their veins of heavenly pays.\\n\\nWARWICK:\\nWhate'er I am hour.\\n\\nCLAUEENCE:\\nO, no, ho, shall our poison three proud cause,\\nShall I of fortune a hopering grave;\\nBut man, a soldier is but a dear\\nAnd happy more than they are cother diding.\\n\\nLADY ANNE:\\nWould I do well be too far gone! What when?\\n\\nKING HENRY VI:\\nWhy, then he singled and forbear.\\n\\nCORIOLANUS:\\nI would not give us here well.\\nO, that in chief enemy thou hast pruck'd toward.\\n\\nRICHARD:\\nAh, if you were a brother, who would they go.\\n\\nKING RICHARD III:\\nEven with the friar, that shall have me while myself\\nMake the hapless maid and well as I.\\nBut, his order, I have stopp'd\\nTo him and wail his thoughts, I forgated\\nTill it was wretched friend petty.\\nCome, Sit in short brings that time who should broke\\nProclaim'd the solemnitange of my poise\\nTo entertuin this kingdom then and tribunes\\nIn thy benefits were eachs good, though beggars\\nAnd left her by Shrift. What compouth it?\\n\\n\"\n",
      " b'ROMEO:\\nPlainly have been by aches, gave King Romeo\\nHere satisfalty I read as if the door to Povert\\nWelcome, you are youring to him and that\\nAgainst a brace of soldiers: but be gone.\\n\\nPETRUCHIO:\\nWhere is he would I cannot make\\nHer widow our good face.\\nPoor birth, call Cofform, this rish of given witness most noble hours,\\nBut Fleshed light, this burd is saved me,\\nHaving my afformity will I do.\\nWhat sayest thou, if they say?\\n\\nThird Servingman:\\nWhere is the poor sirk? come to me for him: put\\nThe fashioner single lives but thine and what he would\\nBurged I am forgive; nor I.\\nCome Two make haste.\\n\\nSEBASTIAN:\\nOn what my weapon, may not, course of yours;\\nWhich we or injustice to his sweet birdons\\nDid them would give the thight, surerty,\\nAnd wet his grief and statutes up my life\\nSubscured with two dies flowers from the vantage liege\\nThan an undiscedide, as you have been more innone,\\nAnd I am cherishe with thee more instruction spoil:\\nAnd now forbidden bands and give this letter; then,\\nso it be the god'\n",
      " b\"ROMEO:\\nThat's tented in the base earth.\\n\\nELIZILA:\\nTake him that he's not tale's a god pleasant;\\nMore inficial fix his son, were butle\\nAnd what of his wits rightly was mine ear, and I\\nWould be husbandry wenched with blood-sting\\nMake war with mine or life: what noise is this\\nDischerish betting what before?\\n\\nTRANIO:\\nIs't possible you will dismiss his native blood?\\n\\nProvost:\\nNone, sir.\\n\\nSomER:\\nIn God's name when, he is touch'd to Montague.\\nWe have an ise it,--whither.\\n\\nGREY:\\nGod well hath Clifford's man, whose notish'd life\\nAnd light than once of his mouth and Better keeps\\nThe princess favour.\\n\\nMIRANDA:\\nSir, thou\\nknow'st\\nthis canst not stand aloud.\\n\\nCORIOLANUS:\\nI will be screcking this: I shall lay him.\\n\\nAUTOLYCUS:\\nNo, I must speak together.\\n\\nDognt\\nIf I had thought, sir, 'tis a safer staring star;\\nThy with thy shooting themselves, Scoluxe and Edward,\\nWill you dispend me to thy breath of us?\\n\\nPRINCE EDWARD:\\nMadam, we did further then my knees he king,\\nthat he does fear the crickets believe me to \"\n",
      " b\"ROMEO:\\nThe concludes are disputeon'd to do this, thou art to many moused\\nWith folly, whom content you to acheep she is\\nan hourly completing. If it be not so grieved with one\\nThat voice more rich for whom I firmly am I dine:\\nTill stame eschpitactors again to join worse;\\nBut, little word obhors, pity.\\n\\nBoatswain:\\nHere, sir! all of him; for whom I not, Warwick and\\nDispace your other and our house,\\nAbout a samilipy of some poison throug-beard's whence?\\nFriar, I know, they scarce bandy till my lany world.\\n\\nDUKE OF YORK:\\nGo, go me thus for laverones; a sweet belly,\\nI must hame you a power, good Brother Margaret was\\nThe princess Baptista course to go with me;\\nMy fault, I will give out Menenius,\\nThrong desire upon this action to thy brother\\nto set the deposing lives for back and leaves of liberty.\\nLet me embrace thee in the chames of soon\\nHer fire and enter there which heaven\\nAs was his heading: but how I give thee,\\nHow ill-seemer that you are like to talk.\\n\\nKING RICHARD II:\\nHead and aphose where I \"], shape=(5,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 3.9220669269561768\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fc436ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x0000022741AFF8B0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x0000022741AFF8B0>, because it is not built.\n",
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d605943b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "How brother Henry, wert too much side\n",
      "Sithen the safer in the noble breasts, stand blows,\n",
      "Congual'd\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f34c10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
